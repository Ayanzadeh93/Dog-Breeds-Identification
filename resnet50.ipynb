{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resenet50+ fintuning last 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCEPTION_v3_starter.ipynb\n",
      "LICENSE\n",
      "README.md\n",
      "Untitled.ipynb\n",
      "VGG19.ipynb\n",
      "custom_layers\n",
      "data_gen\n",
      "dens121.ipynb\n",
      "dense169.ipynb\n",
      "densenet121.py\n",
      "densenet161.py\n",
      "densenet169.py\n",
      "denset201.ipynb\n",
      "googlenet.ipynb\n",
      "googlenet.py\n",
      "imagenet_models\n",
      "inception v3.ipynb\n",
      "inception_v3.py\n",
      "inception_v4.py\n",
      "inception_version4.ipynb\n",
      "keras_VGG19_without pretrain.ipynb\n",
      "labels.csv\n",
      "load_cifar10.py\n",
      "load_cifar10.pyc\n",
      "mixed.ipynb\n",
      "resnet101.ipynb\n",
      "resnet50.ipynb\n",
      "resnet_101.py\n",
      "resnet_152.py\n",
      "resnet_50.py\n",
      "result-2.csv\n",
      "result-3.csv\n",
      "result-4.csv\n",
      "sample_submission.csv\n",
      "test\n",
      "train\n",
      "untitled\n",
      "vgg16.py\n",
      "vgg19.py\n",
      "view\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [01:39<00:00, 102.49it/s]\n",
      "100%|██████████| 10357/10357 [01:39<00:00, 103.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 224, 224, 3)\n",
      "(10222, 120)\n",
      "(10357, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:139: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (7, 7), strides=(2, 2), name=\"conv1\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), strides=(1, 1), name=\"res2a_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"res2a_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res2a_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), strides=(1, 1), name=\"res2a_branch1\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:112: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), name=\"res2b_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"res2b_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res2b_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:74: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), name=\"res2c_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"res2c_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res2c_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), strides=(2, 2), name=\"res3a_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"res3a_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), name=\"res3a_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), strides=(2, 2), name=\"res3a_branch1\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), name=\"res3b_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"res3b_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), name=\"res3b_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), name=\"res3c_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"res3c_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), name=\"res3c_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), name=\"res3d_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"res3d_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), name=\"res3d_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), strides=(2, 2), name=\"res4a_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", name=\"res4a_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), name=\"res4a_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), strides=(2, 2), name=\"res4a_branch1\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res4b_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", name=\"res4b_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), name=\"res4b_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res4c_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", name=\"res4c_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), name=\"res4c_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res4d_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", name=\"res4d_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), name=\"res4d_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res4e_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", name=\"res4e_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), name=\"res4e_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), name=\"res4f_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", name=\"res4f_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), name=\"res4f_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), strides=(2, 2), name=\"res5a_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", name=\"res5a_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (1, 1), name=\"res5a_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (1, 1), strides=(2, 2), name=\"res5a_branch1\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), name=\"res5b_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", name=\"res5b_branch2b\")`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (1, 1), name=\"res5b_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), name=\"res5c_branch2a\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", name=\"res5c_branch2b\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (1, 1), name=\"res5c_branch2c\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:265: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/250\n",
      "9199/9199 [==============================] - 111s 12ms/step - loss: 3.2982 - acc: 0.3082 - val_loss: 5.5196 - val_acc: 0.0176\n",
      "Epoch 2/250\n",
      "9199/9199 [==============================] - 106s 12ms/step - loss: 1.6551 - acc: 0.6305 - val_loss: 1.4110 - val_acc: 0.6217\n",
      "Epoch 3/250\n",
      "9199/9199 [==============================] - 106s 12ms/step - loss: 1.2042 - acc: 0.7198 - val_loss: 1.1618 - val_acc: 0.6764\n",
      "Epoch 4/250\n",
      "9199/9199 [==============================] - 106s 12ms/step - loss: 0.9922 - acc: 0.7646 - val_loss: 1.0656 - val_acc: 0.7116\n",
      "Epoch 5/250\n",
      "9199/9199 [==============================] - 106s 11ms/step - loss: 0.8602 - acc: 0.7940 - val_loss: 1.0012 - val_acc: 0.7107\n",
      "Epoch 6/250\n",
      "9199/9199 [==============================] - 206s 22ms/step - loss: 0.7684 - acc: 0.8163 - val_loss: 0.9492 - val_acc: 0.7292\n",
      "Epoch 7/250\n",
      "9199/9199 [==============================] - 135s 15ms/step - loss: 0.6866 - acc: 0.8381 - val_loss: 0.9377 - val_acc: 0.7361\n",
      "Epoch 8/250\n",
      "9199/9199 [==============================] - 106s 12ms/step - loss: 0.6380 - acc: 0.8501 - val_loss: 0.9133 - val_acc: 0.7380\n",
      "Epoch 9/250\n",
      "9199/9199 [==============================] - 102s 11ms/step - loss: 0.5863 - acc: 0.8618 - val_loss: 0.8981 - val_acc: 0.7439\n",
      "Epoch 10/250\n",
      "9199/9199 [==============================] - 370s 40ms/step - loss: 0.5489 - acc: 0.8748 - val_loss: 0.8840 - val_acc: 0.7439\n",
      "Epoch 11/250\n",
      "9199/9199 [==============================] - 150s 16ms/step - loss: 0.5088 - acc: 0.8877 - val_loss: 0.8829 - val_acc: 0.7400\n",
      "Epoch 12/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.4817 - acc: 0.8924 - val_loss: 0.8753 - val_acc: 0.7576\n",
      "Epoch 13/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.4552 - acc: 0.8986 - val_loss: 0.8603 - val_acc: 0.7488\n",
      "Epoch 14/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.4272 - acc: 0.9096 - val_loss: 0.8513 - val_acc: 0.7488\n",
      "Epoch 15/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.4107 - acc: 0.9147 - val_loss: 0.8612 - val_acc: 0.7498\n",
      "Epoch 16/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.3897 - acc: 0.9181 - val_loss: 0.8564 - val_acc: 0.7458\n",
      "Epoch 17/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.3736 - acc: 0.9216 - val_loss: 0.8535 - val_acc: 0.7527\n",
      "Epoch 18/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.3595 - acc: 0.9289 - val_loss: 0.8431 - val_acc: 0.7537\n",
      "Epoch 19/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.3423 - acc: 0.9325 - val_loss: 0.8430 - val_acc: 0.7576\n",
      "Epoch 20/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.3266 - acc: 0.9398 - val_loss: 0.8437 - val_acc: 0.7468\n",
      "Epoch 21/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.3134 - acc: 0.9447 - val_loss: 0.8478 - val_acc: 0.7546\n",
      "Epoch 22/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.2958 - acc: 0.9465 - val_loss: 0.8402 - val_acc: 0.7468\n",
      "Epoch 23/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.2855 - acc: 0.9502 - val_loss: 0.8536 - val_acc: 0.7537\n",
      "Epoch 24/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.2767 - acc: 0.9528 - val_loss: 0.8504 - val_acc: 0.7527\n",
      "Epoch 25/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.2672 - acc: 0.9552 - val_loss: 0.8483 - val_acc: 0.7537\n",
      "Epoch 26/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.2623 - acc: 0.9559 - val_loss: 0.8441 - val_acc: 0.7586\n",
      "Epoch 27/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.2547 - acc: 0.9577 - val_loss: 0.8514 - val_acc: 0.7449\n",
      "Epoch 28/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.2402 - acc: 0.9624 - val_loss: 0.8484 - val_acc: 0.7556\n",
      "Epoch 29/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.2393 - acc: 0.9625 - val_loss: 0.8428 - val_acc: 0.7586\n",
      "Epoch 30/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.2278 - acc: 0.9659 - val_loss: 0.8441 - val_acc: 0.7566\n",
      "Epoch 31/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.2217 - acc: 0.9652 - val_loss: 0.8408 - val_acc: 0.7537\n",
      "Epoch 32/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.2140 - acc: 0.9677 - val_loss: 0.8487 - val_acc: 0.7527\n",
      "Epoch 33/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.2139 - acc: 0.9672 - val_loss: 0.8471 - val_acc: 0.7498\n",
      "Epoch 34/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.2045 - acc: 0.9705 - val_loss: 0.8517 - val_acc: 0.7625\n",
      "Epoch 35/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1957 - acc: 0.9742 - val_loss: 0.8533 - val_acc: 0.7537\n",
      "Epoch 36/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1912 - acc: 0.9729 - val_loss: 0.8547 - val_acc: 0.7517\n",
      "Epoch 37/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1914 - acc: 0.9731 - val_loss: 0.8570 - val_acc: 0.7576\n",
      "Epoch 38/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1885 - acc: 0.9740 - val_loss: 0.8634 - val_acc: 0.7527\n",
      "Epoch 39/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1838 - acc: 0.9753 - val_loss: 0.8590 - val_acc: 0.7537\n",
      "Epoch 40/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1748 - acc: 0.9770 - val_loss: 0.8609 - val_acc: 0.7537\n",
      "Epoch 41/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1709 - acc: 0.9776 - val_loss: 0.8601 - val_acc: 0.7625\n",
      "Epoch 42/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1676 - acc: 0.9774 - val_loss: 0.8600 - val_acc: 0.7556\n",
      "Epoch 43/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1629 - acc: 0.9801 - val_loss: 0.8666 - val_acc: 0.7527\n",
      "Epoch 44/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1644 - acc: 0.9797 - val_loss: 0.8686 - val_acc: 0.7595\n",
      "Epoch 45/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1604 - acc: 0.9804 - val_loss: 0.8671 - val_acc: 0.7576\n",
      "Epoch 46/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1517 - acc: 0.9821 - val_loss: 0.8702 - val_acc: 0.7507\n",
      "Epoch 47/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1524 - acc: 0.9815 - val_loss: 0.8580 - val_acc: 0.7586\n",
      "Epoch 48/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1510 - acc: 0.9826 - val_loss: 0.8690 - val_acc: 0.7566\n",
      "Epoch 49/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.1487 - acc: 0.9822 - val_loss: 0.8708 - val_acc: 0.7586\n",
      "Epoch 50/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1450 - acc: 0.9843 - val_loss: 0.8558 - val_acc: 0.7634\n",
      "Epoch 51/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1395 - acc: 0.9839 - val_loss: 0.8747 - val_acc: 0.7537\n",
      "Epoch 52/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1392 - acc: 0.9858 - val_loss: 0.8714 - val_acc: 0.7595\n",
      "Epoch 53/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1380 - acc: 0.9839 - val_loss: 0.8730 - val_acc: 0.7527\n",
      "Epoch 54/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1363 - acc: 0.9834 - val_loss: 0.8662 - val_acc: 0.7458\n",
      "Epoch 55/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1323 - acc: 0.9855 - val_loss: 0.8712 - val_acc: 0.7556\n",
      "Epoch 56/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1301 - acc: 0.9870 - val_loss: 0.8686 - val_acc: 0.7566\n",
      "Epoch 57/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1287 - acc: 0.9871 - val_loss: 0.8737 - val_acc: 0.7546\n",
      "Epoch 58/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.1250 - acc: 0.9874 - val_loss: 0.8692 - val_acc: 0.7615\n",
      "Epoch 59/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1206 - acc: 0.9889 - val_loss: 0.8810 - val_acc: 0.7537\n",
      "Epoch 60/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1212 - acc: 0.9891 - val_loss: 0.8808 - val_acc: 0.7566\n",
      "Epoch 61/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1226 - acc: 0.9863 - val_loss: 0.8766 - val_acc: 0.7537\n",
      "Epoch 62/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.1172 - acc: 0.9882 - val_loss: 0.8785 - val_acc: 0.7517\n",
      "Epoch 63/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1163 - acc: 0.9877 - val_loss: 0.8865 - val_acc: 0.7527\n",
      "Epoch 64/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.1139 - acc: 0.9875 - val_loss: 0.8878 - val_acc: 0.7566\n",
      "Epoch 65/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.1121 - acc: 0.9900 - val_loss: 0.8853 - val_acc: 0.7488\n",
      "Epoch 66/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1127 - acc: 0.9889 - val_loss: 0.8820 - val_acc: 0.7449\n",
      "Epoch 67/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1075 - acc: 0.9915 - val_loss: 0.8808 - val_acc: 0.7507\n",
      "Epoch 68/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1113 - acc: 0.9892 - val_loss: 0.8847 - val_acc: 0.7537\n",
      "Epoch 69/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1081 - acc: 0.9893 - val_loss: 0.8814 - val_acc: 0.7527\n",
      "Epoch 70/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.1074 - acc: 0.9895 - val_loss: 0.8895 - val_acc: 0.7507\n",
      "Epoch 71/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.1062 - acc: 0.9903 - val_loss: 0.8814 - val_acc: 0.7517\n",
      "Epoch 72/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0990 - acc: 0.9908 - val_loss: 0.8835 - val_acc: 0.7566\n",
      "Epoch 73/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.1035 - acc: 0.9901 - val_loss: 0.8895 - val_acc: 0.7498\n",
      "Epoch 74/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0983 - acc: 0.9928 - val_loss: 0.8852 - val_acc: 0.7517\n",
      "Epoch 75/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0987 - acc: 0.9924 - val_loss: 0.8808 - val_acc: 0.7537\n",
      "Epoch 76/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0955 - acc: 0.9925 - val_loss: 0.8825 - val_acc: 0.7546\n",
      "Epoch 77/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0997 - acc: 0.9913 - val_loss: 0.8922 - val_acc: 0.7507\n",
      "Epoch 78/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0933 - acc: 0.9935 - val_loss: 0.8930 - val_acc: 0.7576\n",
      "Epoch 79/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0952 - acc: 0.9911 - val_loss: 0.8887 - val_acc: 0.7576\n",
      "Epoch 80/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0967 - acc: 0.9910 - val_loss: 0.8902 - val_acc: 0.7527\n",
      "Epoch 81/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0943 - acc: 0.9909 - val_loss: 0.8932 - val_acc: 0.7507\n",
      "Epoch 82/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0921 - acc: 0.9937 - val_loss: 0.8905 - val_acc: 0.7546\n",
      "Epoch 83/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0920 - acc: 0.9922 - val_loss: 0.8870 - val_acc: 0.7527\n",
      "Epoch 84/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0865 - acc: 0.9932 - val_loss: 0.8942 - val_acc: 0.7527\n",
      "Epoch 85/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0883 - acc: 0.9929 - val_loss: 0.8910 - val_acc: 0.7556\n",
      "Epoch 86/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0856 - acc: 0.9933 - val_loss: 0.8947 - val_acc: 0.7478\n",
      "Epoch 87/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0893 - acc: 0.9936 - val_loss: 0.8917 - val_acc: 0.7527\n",
      "Epoch 88/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0833 - acc: 0.9934 - val_loss: 0.9093 - val_acc: 0.7478\n",
      "Epoch 89/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0846 - acc: 0.9925 - val_loss: 0.9023 - val_acc: 0.7605\n",
      "Epoch 90/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0859 - acc: 0.9921 - val_loss: 0.9014 - val_acc: 0.7507\n",
      "Epoch 91/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0856 - acc: 0.9936 - val_loss: 0.9031 - val_acc: 0.7527\n",
      "Epoch 92/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0839 - acc: 0.9938 - val_loss: 0.9102 - val_acc: 0.7537\n",
      "Epoch 93/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0828 - acc: 0.9925 - val_loss: 0.9074 - val_acc: 0.7488\n",
      "Epoch 94/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0789 - acc: 0.9940 - val_loss: 0.9021 - val_acc: 0.7576\n",
      "Epoch 95/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0800 - acc: 0.9940 - val_loss: 0.9050 - val_acc: 0.7527\n",
      "Epoch 96/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0778 - acc: 0.9943 - val_loss: 0.9062 - val_acc: 0.7517\n",
      "Epoch 97/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0792 - acc: 0.9939 - val_loss: 0.8996 - val_acc: 0.7537\n",
      "Epoch 98/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0764 - acc: 0.9941 - val_loss: 0.9031 - val_acc: 0.7537\n",
      "Epoch 99/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0794 - acc: 0.9930 - val_loss: 0.9031 - val_acc: 0.7527\n",
      "Epoch 100/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0772 - acc: 0.9940 - val_loss: 0.9001 - val_acc: 0.7546\n",
      "Epoch 101/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0758 - acc: 0.9932 - val_loss: 0.9057 - val_acc: 0.7507\n",
      "Epoch 102/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0731 - acc: 0.9949 - val_loss: 0.9062 - val_acc: 0.7507\n",
      "Epoch 103/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0756 - acc: 0.9946 - val_loss: 0.9097 - val_acc: 0.7556\n",
      "Epoch 104/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0747 - acc: 0.9937 - val_loss: 0.9074 - val_acc: 0.7498\n",
      "Epoch 105/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0725 - acc: 0.9950 - val_loss: 0.9148 - val_acc: 0.7527\n",
      "Epoch 106/250\n",
      "9199/9199 [==============================] - 115s 12ms/step - loss: 0.0735 - acc: 0.9941 - val_loss: 0.9065 - val_acc: 0.7517\n",
      "Epoch 107/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0743 - acc: 0.9942 - val_loss: 0.9096 - val_acc: 0.7537\n",
      "Epoch 108/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0734 - acc: 0.9934 - val_loss: 0.9163 - val_acc: 0.7507\n",
      "Epoch 109/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0683 - acc: 0.9954 - val_loss: 0.9107 - val_acc: 0.7527\n",
      "Epoch 110/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0711 - acc: 0.9947 - val_loss: 0.9119 - val_acc: 0.7546\n",
      "Epoch 111/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0695 - acc: 0.9958 - val_loss: 0.9236 - val_acc: 0.7517\n",
      "Epoch 112/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0727 - acc: 0.9939 - val_loss: 0.9175 - val_acc: 0.7517\n",
      "Epoch 113/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0695 - acc: 0.9949 - val_loss: 0.9142 - val_acc: 0.7527\n",
      "Epoch 114/250\n",
      "9199/9199 [==============================] - 143s 15ms/step - loss: 0.0705 - acc: 0.9942 - val_loss: 0.9199 - val_acc: 0.7556\n",
      "Epoch 115/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0667 - acc: 0.9952 - val_loss: 0.9171 - val_acc: 0.7517\n",
      "Epoch 116/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0674 - acc: 0.9959 - val_loss: 0.9196 - val_acc: 0.7498\n",
      "Epoch 117/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0655 - acc: 0.9955 - val_loss: 0.9247 - val_acc: 0.7537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0675 - acc: 0.9952 - val_loss: 0.9189 - val_acc: 0.7507\n",
      "Epoch 119/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0659 - acc: 0.9948 - val_loss: 0.9188 - val_acc: 0.7556\n",
      "Epoch 120/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0654 - acc: 0.9963 - val_loss: 0.9200 - val_acc: 0.7527\n",
      "Epoch 121/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0642 - acc: 0.9951 - val_loss: 0.9252 - val_acc: 0.7537\n",
      "Epoch 122/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0635 - acc: 0.9964 - val_loss: 0.9230 - val_acc: 0.7498\n",
      "Epoch 123/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0671 - acc: 0.9946 - val_loss: 0.9215 - val_acc: 0.7488\n",
      "Epoch 124/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0662 - acc: 0.9954 - val_loss: 0.9216 - val_acc: 0.7507\n",
      "Epoch 125/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0646 - acc: 0.9951 - val_loss: 0.9292 - val_acc: 0.7488\n",
      "Epoch 126/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0625 - acc: 0.9963 - val_loss: 0.9174 - val_acc: 0.7517\n",
      "Epoch 127/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0607 - acc: 0.9960 - val_loss: 0.9181 - val_acc: 0.7517\n",
      "Epoch 128/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0621 - acc: 0.9962 - val_loss: 0.9266 - val_acc: 0.7527\n",
      "Epoch 129/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0607 - acc: 0.9968 - val_loss: 0.9274 - val_acc: 0.7449\n",
      "Epoch 130/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0609 - acc: 0.9964 - val_loss: 0.9086 - val_acc: 0.7488\n",
      "Epoch 131/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0602 - acc: 0.9964 - val_loss: 0.9230 - val_acc: 0.7507\n",
      "Epoch 132/250\n",
      "9199/9199 [==============================] - 143s 15ms/step - loss: 0.0597 - acc: 0.9955 - val_loss: 0.9226 - val_acc: 0.7527\n",
      "Epoch 133/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0602 - acc: 0.9965 - val_loss: 0.9294 - val_acc: 0.7488\n",
      "Epoch 134/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0589 - acc: 0.9961 - val_loss: 0.9242 - val_acc: 0.7488\n",
      "Epoch 135/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0592 - acc: 0.9963 - val_loss: 0.9307 - val_acc: 0.7537\n",
      "Epoch 136/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0588 - acc: 0.9967 - val_loss: 0.9235 - val_acc: 0.7498\n",
      "Epoch 137/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0566 - acc: 0.9957 - val_loss: 0.9264 - val_acc: 0.7527\n",
      "Epoch 138/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0567 - acc: 0.9970 - val_loss: 0.9280 - val_acc: 0.7498\n",
      "Epoch 139/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0581 - acc: 0.9960 - val_loss: 0.9344 - val_acc: 0.7488\n",
      "Epoch 140/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0570 - acc: 0.9960 - val_loss: 0.9290 - val_acc: 0.7546\n",
      "Epoch 141/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0587 - acc: 0.9962 - val_loss: 0.9276 - val_acc: 0.7586\n",
      "Epoch 142/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0562 - acc: 0.9972 - val_loss: 0.9247 - val_acc: 0.7556\n",
      "Epoch 143/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0560 - acc: 0.9960 - val_loss: 0.9300 - val_acc: 0.7527\n",
      "Epoch 144/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0553 - acc: 0.9975 - val_loss: 0.9394 - val_acc: 0.7546\n",
      "Epoch 145/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0555 - acc: 0.9958 - val_loss: 0.9323 - val_acc: 0.7498\n",
      "Epoch 146/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0540 - acc: 0.9966 - val_loss: 0.9285 - val_acc: 0.7478\n",
      "Epoch 147/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0534 - acc: 0.9973 - val_loss: 0.9426 - val_acc: 0.7498\n",
      "Epoch 148/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0545 - acc: 0.9955 - val_loss: 0.9335 - val_acc: 0.7537\n",
      "Epoch 149/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0523 - acc: 0.9974 - val_loss: 0.9383 - val_acc: 0.7507\n",
      "Epoch 150/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0531 - acc: 0.9968 - val_loss: 0.9326 - val_acc: 0.7517\n",
      "Epoch 151/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0518 - acc: 0.9965 - val_loss: 0.9342 - val_acc: 0.7527\n",
      "Epoch 152/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0511 - acc: 0.9977 - val_loss: 0.9322 - val_acc: 0.7488\n",
      "Epoch 153/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0527 - acc: 0.9958 - val_loss: 0.9324 - val_acc: 0.7488\n",
      "Epoch 154/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0526 - acc: 0.9968 - val_loss: 0.9455 - val_acc: 0.7488\n",
      "Epoch 155/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0528 - acc: 0.9961 - val_loss: 0.9400 - val_acc: 0.7498\n",
      "Epoch 156/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0513 - acc: 0.9972 - val_loss: 0.9346 - val_acc: 0.7507\n",
      "Epoch 157/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0529 - acc: 0.9968 - val_loss: 0.9361 - val_acc: 0.7527\n",
      "Epoch 158/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0507 - acc: 0.9977 - val_loss: 0.9396 - val_acc: 0.7527\n",
      "Epoch 159/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0495 - acc: 0.9967 - val_loss: 0.9322 - val_acc: 0.7507\n",
      "Epoch 160/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0496 - acc: 0.9962 - val_loss: 0.9365 - val_acc: 0.7517\n",
      "Epoch 161/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0520 - acc: 0.9962 - val_loss: 0.9369 - val_acc: 0.7488\n",
      "Epoch 162/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0504 - acc: 0.9963 - val_loss: 0.9357 - val_acc: 0.7566\n",
      "Epoch 163/250\n",
      "9199/9199 [==============================] - 143s 15ms/step - loss: 0.0525 - acc: 0.9958 - val_loss: 0.9444 - val_acc: 0.7537\n",
      "Epoch 164/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0497 - acc: 0.9971 - val_loss: 0.9411 - val_acc: 0.7586\n",
      "Epoch 165/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0493 - acc: 0.9972 - val_loss: 0.9386 - val_acc: 0.7546\n",
      "Epoch 166/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0494 - acc: 0.9967 - val_loss: 0.9334 - val_acc: 0.7498\n",
      "Epoch 167/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0491 - acc: 0.9966 - val_loss: 0.9396 - val_acc: 0.7507\n",
      "Epoch 168/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0491 - acc: 0.9970 - val_loss: 0.9446 - val_acc: 0.7556\n",
      "Epoch 169/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0476 - acc: 0.9972 - val_loss: 0.9375 - val_acc: 0.7546\n",
      "Epoch 170/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0468 - acc: 0.9963 - val_loss: 0.9398 - val_acc: 0.7586\n",
      "Epoch 171/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0478 - acc: 0.9971 - val_loss: 0.9457 - val_acc: 0.7478\n",
      "Epoch 172/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0467 - acc: 0.9972 - val_loss: 0.9417 - val_acc: 0.7488\n",
      "Epoch 173/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0478 - acc: 0.9964 - val_loss: 0.9407 - val_acc: 0.7546\n",
      "Epoch 174/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0474 - acc: 0.9972 - val_loss: 0.9417 - val_acc: 0.7537\n",
      "Epoch 175/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0479 - acc: 0.9973 - val_loss: 0.9442 - val_acc: 0.7546\n",
      "Epoch 176/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0472 - acc: 0.9970 - val_loss: 0.9418 - val_acc: 0.7517\n",
      "Epoch 177/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0461 - acc: 0.9966 - val_loss: 0.9444 - val_acc: 0.7507\n",
      "Epoch 178/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0462 - acc: 0.9971 - val_loss: 0.9466 - val_acc: 0.7556\n",
      "Epoch 179/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0466 - acc: 0.9965 - val_loss: 0.9450 - val_acc: 0.7566\n",
      "Epoch 180/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0446 - acc: 0.9967 - val_loss: 0.9492 - val_acc: 0.7576\n",
      "Epoch 181/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0442 - acc: 0.9971 - val_loss: 0.9384 - val_acc: 0.7546\n",
      "Epoch 182/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0469 - acc: 0.9954 - val_loss: 0.9529 - val_acc: 0.7517\n",
      "Epoch 183/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0464 - acc: 0.9974 - val_loss: 0.9511 - val_acc: 0.7517\n",
      "Epoch 184/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0437 - acc: 0.9973 - val_loss: 0.9479 - val_acc: 0.7517\n",
      "Epoch 185/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0437 - acc: 0.9975 - val_loss: 0.9519 - val_acc: 0.7478\n",
      "Epoch 186/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0449 - acc: 0.9972 - val_loss: 0.9533 - val_acc: 0.7498\n",
      "Epoch 187/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0442 - acc: 0.9977 - val_loss: 0.9487 - val_acc: 0.7527\n",
      "Epoch 188/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0445 - acc: 0.9966 - val_loss: 0.9416 - val_acc: 0.7488\n",
      "Epoch 189/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0444 - acc: 0.9973 - val_loss: 0.9451 - val_acc: 0.7507\n",
      "Epoch 190/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0430 - acc: 0.9975 - val_loss: 0.9471 - val_acc: 0.7527\n",
      "Epoch 191/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0426 - acc: 0.9976 - val_loss: 0.9502 - val_acc: 0.7507\n",
      "Epoch 192/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0409 - acc: 0.9984 - val_loss: 0.9494 - val_acc: 0.7488\n",
      "Epoch 193/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0452 - acc: 0.9972 - val_loss: 0.9562 - val_acc: 0.7527\n",
      "Epoch 194/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0425 - acc: 0.9974 - val_loss: 0.9519 - val_acc: 0.7498\n",
      "Epoch 195/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0424 - acc: 0.9970 - val_loss: 0.9586 - val_acc: 0.7488\n",
      "Epoch 196/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0433 - acc: 0.9974 - val_loss: 0.9501 - val_acc: 0.7537\n",
      "Epoch 197/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0413 - acc: 0.9977 - val_loss: 0.9547 - val_acc: 0.7507\n",
      "Epoch 198/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0421 - acc: 0.9977 - val_loss: 0.9574 - val_acc: 0.7527\n",
      "Epoch 199/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0444 - acc: 0.9974 - val_loss: 0.9549 - val_acc: 0.7527\n",
      "Epoch 200/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0406 - acc: 0.9976 - val_loss: 0.9539 - val_acc: 0.7517\n",
      "Epoch 201/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0429 - acc: 0.9977 - val_loss: 0.9631 - val_acc: 0.7488\n",
      "Epoch 202/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0423 - acc: 0.9973 - val_loss: 0.9604 - val_acc: 0.7478\n",
      "Epoch 203/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0408 - acc: 0.9973 - val_loss: 0.9567 - val_acc: 0.7507\n",
      "Epoch 204/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0408 - acc: 0.9977 - val_loss: 0.9556 - val_acc: 0.7498\n",
      "Epoch 205/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0416 - acc: 0.9978 - val_loss: 0.9567 - val_acc: 0.7498\n",
      "Epoch 206/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0413 - acc: 0.9975 - val_loss: 0.9548 - val_acc: 0.7546\n",
      "Epoch 207/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0403 - acc: 0.9973 - val_loss: 0.9528 - val_acc: 0.7517\n",
      "Epoch 208/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0409 - acc: 0.9978 - val_loss: 0.9521 - val_acc: 0.7517\n",
      "Epoch 209/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0424 - acc: 0.9970 - val_loss: 0.9611 - val_acc: 0.7498\n",
      "Epoch 210/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0400 - acc: 0.9968 - val_loss: 0.9597 - val_acc: 0.7507\n",
      "Epoch 211/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0396 - acc: 0.9966 - val_loss: 0.9620 - val_acc: 0.7507\n",
      "Epoch 212/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0391 - acc: 0.9972 - val_loss: 0.9626 - val_acc: 0.7537\n",
      "Epoch 213/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0405 - acc: 0.9972 - val_loss: 0.9556 - val_acc: 0.7537\n",
      "Epoch 214/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0382 - acc: 0.9985 - val_loss: 0.9620 - val_acc: 0.7527\n",
      "Epoch 215/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0411 - acc: 0.9964 - val_loss: 0.9672 - val_acc: 0.7556\n",
      "Epoch 216/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0396 - acc: 0.9977 - val_loss: 0.9641 - val_acc: 0.7507\n",
      "Epoch 217/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0398 - acc: 0.9975 - val_loss: 0.9702 - val_acc: 0.7498\n",
      "Epoch 218/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0394 - acc: 0.9976 - val_loss: 0.9621 - val_acc: 0.7517\n",
      "Epoch 219/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0385 - acc: 0.9974 - val_loss: 0.9580 - val_acc: 0.7498\n",
      "Epoch 220/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0402 - acc: 0.9975 - val_loss: 0.9636 - val_acc: 0.7537\n",
      "Epoch 221/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0382 - acc: 0.9975 - val_loss: 0.9592 - val_acc: 0.7556\n",
      "Epoch 222/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0379 - acc: 0.9979 - val_loss: 0.9620 - val_acc: 0.7468\n",
      "Epoch 223/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0364 - acc: 0.9978 - val_loss: 0.9636 - val_acc: 0.7546\n",
      "Epoch 224/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0382 - acc: 0.9976 - val_loss: 0.9663 - val_acc: 0.7566\n",
      "Epoch 225/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0389 - acc: 0.9975 - val_loss: 0.9677 - val_acc: 0.7527\n",
      "Epoch 226/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0363 - acc: 0.9980 - val_loss: 0.9631 - val_acc: 0.7566\n",
      "Epoch 227/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0377 - acc: 0.9976 - val_loss: 0.9680 - val_acc: 0.7498\n",
      "Epoch 228/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0377 - acc: 0.9980 - val_loss: 0.9587 - val_acc: 0.7527\n",
      "Epoch 229/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0373 - acc: 0.9971 - val_loss: 0.9654 - val_acc: 0.7527\n",
      "Epoch 230/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0375 - acc: 0.9973 - val_loss: 0.9716 - val_acc: 0.7498\n",
      "Epoch 231/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0377 - acc: 0.9977 - val_loss: 0.9695 - val_acc: 0.7498\n",
      "Epoch 232/250\n",
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0384 - acc: 0.9974 - val_loss: 0.9689 - val_acc: 0.7517\n",
      "Epoch 233/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0383 - acc: 0.9968 - val_loss: 0.9731 - val_acc: 0.7517\n",
      "Epoch 234/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - 144s 16ms/step - loss: 0.0379 - acc: 0.9976 - val_loss: 0.9736 - val_acc: 0.7498\n",
      "Epoch 235/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0367 - acc: 0.9983 - val_loss: 0.9680 - val_acc: 0.7537\n",
      "Epoch 236/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0367 - acc: 0.9979 - val_loss: 0.9725 - val_acc: 0.7507\n",
      "Epoch 237/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0367 - acc: 0.9974 - val_loss: 0.9653 - val_acc: 0.7556\n",
      "Epoch 238/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0352 - acc: 0.9977 - val_loss: 0.9686 - val_acc: 0.7546\n",
      "Epoch 239/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0364 - acc: 0.9976 - val_loss: 0.9768 - val_acc: 0.7498\n",
      "Epoch 240/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0351 - acc: 0.9977 - val_loss: 0.9604 - val_acc: 0.7537\n",
      "Epoch 241/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0385 - acc: 0.9970 - val_loss: 0.9775 - val_acc: 0.7517\n",
      "Epoch 242/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0358 - acc: 0.9979 - val_loss: 0.9749 - val_acc: 0.7468\n",
      "Epoch 243/250\n",
      "9199/9199 [==============================] - 142s 15ms/step - loss: 0.0358 - acc: 0.9980 - val_loss: 0.9806 - val_acc: 0.7537\n",
      "Epoch 244/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0366 - acc: 0.9982 - val_loss: 0.9779 - val_acc: 0.7527\n",
      "Epoch 245/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0355 - acc: 0.9979 - val_loss: 0.9771 - val_acc: 0.7517\n",
      "Epoch 246/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0359 - acc: 0.9982 - val_loss: 0.9723 - val_acc: 0.7517\n",
      "Epoch 247/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0349 - acc: 0.9977 - val_loss: 0.9738 - val_acc: 0.7517\n",
      "Epoch 248/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0350 - acc: 0.9978 - val_loss: 0.9763 - val_acc: 0.7556\n",
      "Epoch 249/250\n",
      "9199/9199 [==============================] - 143s 16ms/step - loss: 0.0358 - acc: 0.9974 - val_loss: 0.9697 - val_acc: 0.7517\n",
      "Epoch 250/250\n",
      "9199/9199 [==============================] - 141s 15ms/step - loss: 0.0354 - acc: 0.9983 - val_loss: 0.9751 - val_acc: 0.7527\n",
      "1023/1023 [==============================] - 16s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, merge, ZeroPadding2D\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from custom_layers.scale_layer import Scale\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"\n",
    "    The identity_block is the block that has no conv layer at shortcut\n",
    "    Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    \"\"\"\n",
    "\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n",
    "                      border_mode='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = merge([x, input_tensor], mode='sum')\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"\n",
    "    conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    \"\"\"\n",
    "\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode='same',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = merge([x, shortcut], mode='sum')\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet50_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
    "    \"\"\"\n",
    "    Resnet 50 Model for Keras\n",
    "    Model Schema is based on \n",
    "    https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n",
    "    ImageNet Pretrained Weights \n",
    "    https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels.h5\n",
    "    Parameters:\n",
    "      img_rows, img_cols - resolution of inputs\n",
    "      channel - 1 for grayscale, 3 for color \n",
    "      num_classes - number of class labels for our classification task\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "      bn_axis = 3\n",
    "      img_input = Input(shape=(img_rows, img_cols, color_type))\n",
    "    else:\n",
    "      bn_axis = 1\n",
    "      img_input = Input(shape=(color_type, img_rows, img_cols))\n",
    "\n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # Fully Connected Softmax Layer\n",
    "    x_fc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    x_fc = Flatten()(x_fc)\n",
    "    x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(img_input, x_fc)\n",
    "\n",
    "    # Load ImageNet pre-trained data \n",
    "    if K.image_dim_ordering() == 'th':\n",
    "      # Use pre-trained weights for Theano backend\n",
    "      weights_path = 'imagenet_models/resnet50_weights_th_dim_ordering_th_kernels.h5'\n",
    "    else:\n",
    "      # Use pre-trained weights for Tensorflow backend\n",
    "      weights_path = 'imagenet_models/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "        \n",
    "    for layer in model.layers[:-2]:\n",
    "        layer.trainable = False\n",
    "    # Truncate and replace softmax layer for transfer learning\n",
    "    # Cannot use model.layers.pop() since model is not of Sequential() type\n",
    "    # The method below works since pre-trained weights are stored in layers but not in the model\n",
    "    x_newfc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    x_newfc = Flatten()(x_newfc)\n",
    "    x_newfc = Dense(num_classes, activation='softmax', name='fc10')(x_newfc)\n",
    "\n",
    "    # Create another model with our customized softmax\n",
    "    model = Model(img_input, x_newfc)\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Example to fine-tune on 3000 samples from Cifar10\n",
    "\n",
    "    img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "    channel = 3\n",
    "    num_classes = 120 \n",
    "    batch_size = 16 \n",
    "    nb_epoch = 250\n",
    "    \n",
    "    directory_path = \"/home/aydin/storage/aydin/kaggle-dog/Kaggle-dog-breeds-classification/cnn_finetune-master/\"\n",
    "\n",
    "    print(check_output([\"ls\", directory_path]).decode(\"utf8\"))\n",
    "\n",
    "    df_train = pd.read_csv(directory_path+'labels.csv')\n",
    "    df_test = pd.read_csv(directory_path+'sample_submission.csv')\n",
    "\n",
    "    targets_series = pd.Series(df_train['breed'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "\n",
    "    im_size = 224\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "  \n",
    "\n",
    "    i = 0 \n",
    "    for f, breed in tqdm(df_train.values):\n",
    "        img = cv2.imread(directory_path+'train/{}.jpg'.format(f))\n",
    "        label = one_hot_labels[i]\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        y_train.append(label)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for f in tqdm(df_test['id'].values):\n",
    "        img = cv2.imread(directory_path+'test/{}.jpg'.format(f))\n",
    "        x_test.append(cv2.resize(img, (im_size, im_size)))\n",
    "\n",
    "    y_train_raw = np.array(y_train, np.uint8)\n",
    "    x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "    x_test  = np.array(x_test, np.float32) / 255.\n",
    "\n",
    "    print(x_train_raw.shape)\n",
    "    print(y_train_raw.shape)\n",
    "    print(x_test.shape)\n",
    "\n",
    "    num_class = y_train_raw.shape[1]\n",
    "\n",
    "\n",
    "    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.1, random_state=1)\n",
    "\n",
    "    # Load our model\n",
    "    model = resnet50_model(img_rows, img_cols, channel, num_classes)\n",
    "\n",
    "    # Start Fine-tuning\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(X_valid, Y_valid),\n",
    "              )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Cross-entropy loss score\n",
    "    score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - 140s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)\n",
    "\n",
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "\n",
    "sub.to_csv(directory_path+\"result-6.csv\", encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine tuning last 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bf79b76a2957>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[0mdirectory_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/home/aydin/storage/aydin/kaggle-dog/Kaggle-dog-breeds-classification/cnn_finetune-master/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'stdout'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stdout argument not allowed, it will be overridden.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[0;32m    388\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                                 errread, errwrite)\n\u001b[0m\u001b[0;32m    391\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mgc_was_enabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#config.gpu_options.visible_device_list = \"0\"\n",
    "#set_session(tf.Session(config=config))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, merge, ZeroPadding2D\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from custom_layers.scale_layer import Scale\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"\n",
    "    The identity_block is the block that has no conv layer at shortcut\n",
    "    Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    \"\"\"\n",
    "\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n",
    "                      border_mode='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = merge([x, input_tensor], mode='sum')\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"\n",
    "    conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    \"\"\"\n",
    "\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode='same',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = merge([x, shortcut], mode='sum')\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet50_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
    "    \"\"\"\n",
    "    Resnet 50 Model for Keras\n",
    "    Model Schema is based on \n",
    "    https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n",
    "    ImageNet Pretrained Weights \n",
    "    https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels.h5\n",
    "    Parameters:\n",
    "      img_rows, img_cols - resolution of inputs\n",
    "      channel - 1 for grayscale, 3 for color \n",
    "      num_classes - number of class labels for our classification task\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "      bn_axis = 3\n",
    "      img_input = Input(shape=(img_rows, img_cols, color_type))\n",
    "    else:\n",
    "      bn_axis = 1\n",
    "      img_input = Input(shape=(color_type, img_rows, img_cols))\n",
    "\n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # Fully Connected Softmax Layer\n",
    "    x_fc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    x_fc = Flatten()(x_fc)\n",
    "    x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(img_input, x_fc)\n",
    "\n",
    "    # Load ImageNet pre-trained data \n",
    "    if K.image_dim_ordering() == 'th':\n",
    "      # Use pre-trained weights for Theano backend\n",
    "      weights_path = 'imagenet_models/resnet50_weights_th_dim_ordering_th_kernels.h5'\n",
    "    else:\n",
    "      # Use pre-trained weights for Tensorflow backend\n",
    "      weights_path = 'imagenet_models/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "        \n",
    "    for layer in model.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "    # Truncate and replace softmax layer for transfer learning\n",
    "    # Cannot use model.layers.pop() since model is not of Sequential() type\n",
    "    # The method below works since pre-trained weights are stored in layers but not in the model\n",
    "    x_newfc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    x_newfc = Flatten()(x_newfc)\n",
    "    x_newfc = Dense(num_classes, activation='softmax', name='fc10')(x_newfc)\n",
    "\n",
    "    # Create another model with our customized softmax\n",
    "    model = Model(img_input, x_newfc)\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Example to fine-tune on 3000 samples from Cifar10\n",
    "\n",
    "    img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "    channel = 3\n",
    "    num_classes = 120 \n",
    "    batch_size = 16 \n",
    "    nb_epoch = 250\n",
    "    \n",
    "    directory_path = \"/home/aydin/storage/aydin/kaggle-dog/Kaggle-dog-breeds-classification/cnn_finetune-master/\"\n",
    "\n",
    "    print(check_output([\"ls\", directory_path]).decode(\"utf8\"))\n",
    "\n",
    "    df_train = pd.read_csv(directory_path+'labels.csv')\n",
    "    df_test = pd.read_csv(directory_path+'sample_submission.csv')\n",
    "\n",
    "    targets_series = pd.Series(df_train['breed'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "\n",
    "    im_size = 224\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "  \n",
    "\n",
    "    i = 0 \n",
    "    for f, breed in tqdm(df_train.values):\n",
    "        img = cv2.imread(directory_path+'train/{}.jpg'.format(f))\n",
    "        label = one_hot_labels[i]\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        y_train.append(label)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for f in tqdm(df_test['id'].values):\n",
    "        img = cv2.imread(directory_path+'test/{}.jpg'.format(f))\n",
    "        x_test.append(cv2.resize(img, (im_size, im_size)))\n",
    "\n",
    "    y_train_raw = np.array(y_train, np.uint8)\n",
    "    x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "    x_test  = np.array(x_test, np.float32) / 255.\n",
    "\n",
    "    print(x_train_raw.shape)\n",
    "    print(y_train_raw.shape)\n",
    "    print(x_test.shape)\n",
    "\n",
    "    num_class = y_train_raw.shape[1]\n",
    "\n",
    "\n",
    "    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.1, random_state=1)\n",
    "\n",
    "    # Load our model\n",
    "    model = resnet50_model(img_rows, img_cols, channel, num_classes)\n",
    "\n",
    "    # Start Fine-tuning\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(X_valid, Y_valid),\n",
    "              )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Cross-entropy loss score\n",
    "    score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - 104s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)\n",
    "\n",
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "\n",
    "sub.to_csv(directory_path+\"result-4.csv\", encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
